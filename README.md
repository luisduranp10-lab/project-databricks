# project-databricks
Proyecto de Azure Databricks Smart Data 2025


## ğŸš€ Arquitectura del ETL

El proyecto implementa un flujo ETL siguiendo buenas prÃ¡cticas de arquitectura de datos:

### **ğŸ”¸ Capa Bronze**
- Ingesta cruda desde fuentes externas.
- Archivos almacenados en external locations:  
  `exlt-raw`, `exlt-bronze`
- Tablas generadas:
  - `bronze.centers`
  - `bronze.ubigeo`

### **ğŸ”¸ Capa Silver**
- Limpieza, normalizaciÃ³n y enriquecimiento.
- External location: `exlt-silver`
- Tabla resultante:
  - `silver.health_centers_ubigeo`

### **ğŸ”¸ Capa Golden**
- Dataset analÃ­tico final para los dashboards.
- External location: `exlt-golden`
- Tabla final:
  - `golden.golden_health_centers_peru`

---

## ğŸ§± Scripts Incluidos

### **ğŸ“Œ /scripts/**
Contiene los archivos SQL necesarios para preparar el ambiente:

- CreaciÃ³n del catÃ¡logo `catalog_dev`
- CreaciÃ³n de schemas:  
  `bronze`, `silver`, `golden`, `exploratory`
- Registro de external locations
- CreaciÃ³n inicial de tablas

Estos scripts deben ejecutarse antes de correr cualquier notebook ETL.

---

## ğŸ” Seguridad â€“ /seguridad/
Incluye los archivos SQL para otorgar permisos:

- GRANTS sobre catÃ¡logo
- GRANTS sobre schemas
- GRANTS sobre external locations
- GRANTS sobre tablas

Estos permisos estÃ¡n diseÃ±ados para roles como:

- `DataEngineers`
- `Analysts`
- `BI_Team`

---

## â™»ï¸ Rollback â€“ /reversion/
Contiene el archivo:

### **`reversion/revoke.sql`**
Este script elimina:

âœ” Tablas lÃ³gicas (bronze, silver, golden)  
âœ” Schemas  
âœ” External locations  
âœ” CatÃ¡logo completo  

Debe usarse Ãºnicamente para revertir despliegues de prueba o restaurar el ambiente desde cero.

---

## ğŸ§© Proceso ETL â€“ /proceso/
Incluye los notebooks convertidos a `.py`:

- `Ingest_ubigeo.py`
- `Ingest_health_centers.py`
- `Transform.py`
- `Load.py`
- `Orquestador.py`

Cada archivo representa una etapa del ETL:

1. **IngestiÃ³n cruda**  
2. **TransformaciÃ³n**  
3. **UniÃ³n de datasets**  
4. **Carga a capa Golden**  

Estos pueden ser invocados de manera secuencial mediante 
**OrquestaciÃ³n del flujo**  

---

## ğŸ“Š Dashboards â€“ /dashboard/
AquÃ­ se almacenan:

- Archivos `.json` exportados desde Power BI
- ImÃ¡genes `.png` de dashboards
- Reportes `.pbix`
- Enlaces guardados en `.txt`

El dashboard final debe consumir la tabla:  
`golden.golden_health_centers_peru`.

---

## ğŸ§¾ Evidencias â€“ /certificaciones/

---

## ğŸ”§ CI/CD â€“ /.github/workflows/
Flujos propuestos:

- ValidaciÃ³n de estructura del repositorio
- Despliegue automÃ¡tico a ambiente de desarrollo
- Opcional: despliegue a producciÃ³n

(Se activarÃ¡ cuando se configure GitHub Actions)

---

## â–¶ï¸ CÃ³mo ejecutar el proyecto

1. **Ejecutar scripts de `/scripts`**  
   - Crear catÃ¡logo, schemas, external locations y tablas base.

2. **Ejecutar los notebooks del ETL desde `/proceso`**  

	Se ejecuta Orquestador.py

   Este Orquesadoor ejecuta en el siguiente orden:
   
   1) Ingest_ubigeo.py  
   2) Ingest_health_centers.py  
   3) Transform.py  
   4) Load.py  

3. **Validar output en tabla Golden**  
   - `catalog_dev.golden.golden_health_centers_peru`

4. **Actualizar dashboard en `/dashboard`**

---

## ğŸ” Rollback completo

Para limpiar todo el ambiente:

```sql
%sql
RUN ./reversion/revoke.sql
