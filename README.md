# project-databricks
Proyecto de Azure Databricks Smart Data 2025

InformaciÃ³n en base a los datasets de la Plataforma Nacional de Datos Abiertos del Gobierno del PerÃº

`hospitalesopendata.csv`
https://www.datosabiertos.gob.pe/dataset/directorio-de-establecimientos-de-salud?utm_source=chatgpt.com

`ubigeo.csv`
https://www.datosabiertos.gob.pe/dataset/ubigeos-c%C3%B3digos-de-ubicaci%C3%B3n-geogr%C3%A1fica-instituto-nacional-de-estad%C3%ADstica-e-inform%C3%A1tica-inei?utm_source=chatgpt.com


## ğŸš€ Arquitectura del ETL

El proyecto implementa un flujo ETL siguiendo buenas prÃ¡cticas de arquitectura de datos:

### **ğŸ”¸ Capa Bronze**
- Ingesta cruda desde fuentes externas.
- Archivos almacenados en external locations:  
  `exlt-raw`, `exlt-bronze`
- Tablas generadas:
  - `bronze.centers`
  - `bronze.ubigeo`

### **ğŸ”¸ Capa Silver**
- Limpieza, normalizaciÃ³n y enriquecimiento.
- External location: `exlt-silver`
- Tabla resultante:
  - `silver.health_centers_ubigeo`

### **ğŸ”¸ Capa Golden**
- Dataset analÃ­tico final para los dashboards.
- External location: `exlt-golden`
- Tabla final:
  - `golden.golden_health_centers_peru`

---

## ğŸ§± Scripts Incluidos

### **ğŸ“Œ /scripts/**
Contiene los archivos SQL necesarios para preparar el ambiente:

- CreaciÃ³n del catÃ¡logo `catalog_dev`
- CreaciÃ³n de schemas:  
  `bronze`, `silver`, `golden`, `exploratory`
- Registro de external locations
- CreaciÃ³n inicial de tablas

Estos scripts deben ejecutarse antes de correr cualquier notebook ETL.

---

## ğŸ” Seguridad â€“ /seguridad/

---

## â™»ï¸ Rollback â€“ /reversion/
Contiene el archivo:

### **`reversion/revoke.sql`**
Este script elimina:

âœ” Tablas lÃ³gicas (bronze, silver, golden)  
âœ” Schemas  
âœ” External locations  
âœ” CatÃ¡logo completo  

Debe usarse Ãºnicamente para revertir despliegues de prueba o restaurar el ambiente desde cero.

---

## ğŸ§© Proceso ETL â€“ /proceso/
Incluye los notebooks convertidos a `.py`:

- `Ingest_ubigeo.py`
- `Ingest_health_centers.py`
- `Transform.py`
- `Load.py`
- `Orquestador.py`

Cada archivo representa una etapa del ETL:

1. **IngestiÃ³n cruda**  
	- Ingest_ubigeo.py
	- Ingest_health_centers.py
2. **TransformaciÃ³n** 
3. **UniÃ³n de datasets**  
	- Transform.py
4. **Carga a capa Golden** 
	- Load.py

Estos pueden ser invocados de manera secuencial mediante 
**OrquestaciÃ³n del flujo** mediante `Orquestador.py`

---

## ğŸ“Š Dashboards â€“ /dashboard/

- Reporte en PowerBI: Centros_salud_Peru.pbix

El dashboard final consume la tabla:  
`golden.golden_health_centers_peru`.

---

## ğŸ§¾ Evidencias â€“ /certificaciones/

---

## ğŸ”§ CI/CD â€“ /.github/workflows/
Flujos propuestos:

- ValidaciÃ³n de estructura del repositorio
- Despliegue automÃ¡tico a ambiente de desarrollo
- Opcional: despliegue a producciÃ³n

(Se activarÃ¡ cuando se configure GitHub Actions)

---

## â–¶ï¸ CÃ³mo ejecutar el proyecto

1. **Ejecutar scripts de `/scripts`**  
   - Crear catÃ¡logo, schemas, external locations y tablas base.

2. **Ejecutar los notebooks del ETL desde `/proceso`**  

	Se ejecuta Orquestador.py

   Este Orquesadoor ejecuta en el siguiente orden:
   
   1) Ingest_ubigeo.py  
   2) Ingest_health_centers.py  
   3) Transform.py  
   4) Load.py  

3. **Validar output en tabla Golden**  
   - `catalog_dev.golden.golden_health_centers_peru`

4. **Actualizar dashboard en `/dashboard`**

---

## ğŸ” Rollback completo

Para limpiar todo el ambiente:

```sql
%sql
RUN ./reversion/revoke.sql
